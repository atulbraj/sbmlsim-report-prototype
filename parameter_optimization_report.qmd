---
title: "Parameter Optimization Report"
subtitle: "Automated Report Generation for sbmlsim"
author: "Generated by sbmlsim reporting module"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    embed-resources: true
    fig-width: 10
    fig-height: 6
execute:
  echo: false
  warning: false
---

```{python}
#| label: setup
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from load_results import load_optimization_results, get_parameter_table, generate_fit_data

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 11

# Load data
data = load_optimization_results()
```

## Executive Summary

```{python}
#| label: summary-table
#| tbl-cap: "Optimization Summary"

summary_data = {
    'Metric': ['AIC', 'RMSE', 'R²', 'Training RMSE', 'Validation RMSE', 'Iterations'],
    'Value': [
        f"{data['optimization_results']['aic']:.2f}",
        f"{data['optimization_results']['rmse']:.4f}",
        f"{data['optimization_results']['r_squared']:.3f}",
        f"{data['optimization_results']['training_rmse']:.4f}",
        f"{data['optimization_results']['validation_rmse']:.4f}",
        data['optimization_results']['iterations']
    ]
}
summary_df = pd.DataFrame(summary_data)
print(summary_df.to_markdown(index=False))
```

**Model:** `{python} data['metadata']['model_id']`  
**Method:** `{python} data['metadata']['method']`  
**Software:** `{python} data['metadata']['software']`  
**Timestamp:** `{python} data['metadata']['timestamp']`

---

## Optimized Parameters

```{python}
#| label: parameter-table
#| tbl-cap: "Estimated Parameters with 95% Confidence Intervals"

params_df = get_parameter_table(data)
display_df = params_df[['name', 'value', 'unit', 'confidence_interval']].copy()
display_df['CI Lower'] = display_df['confidence_interval'].apply(lambda x: f"{x[0]:.4f}")
display_df['CI Upper'] = display_df['confidence_interval'].apply(lambda x: f"{x[1]:.4f}")
display_df['Value'] = display_df['value'].apply(lambda x: f"{x:.4f}")
display_df = display_df[['name', 'Value', 'CI Lower', 'CI Upper', 'unit']]
display_df.columns = ['Parameter', 'Estimate', 'CI Lower', 'CI Upper', 'Unit']
print(display_df.to_markdown(index=False))
```

```{python}
#| label: parameter-plot
#| fig-cap: "Parameter Estimates with 95% Confidence Intervals"

fig, ax = plt.subplots(figsize=(10, 6))

params_df = get_parameter_table(data)
params_df['ci_lower'] = params_df['confidence_interval'].apply(lambda x: x[0])
params_df['ci_upper'] = params_df['confidence_interval'].apply(lambda x: x[1])
params_df['ci_error_lower'] = params_df['value'] - params_df['ci_lower']
params_df['ci_error_upper'] = params_df['ci_upper'] - params_df['value']

errors = np.array([[row['ci_error_lower'], row['ci_error_upper']] for _, row in params_df.iterrows()]).T

colors = sns.color_palette("husl", len(params_df))
ax.errorbar(
    range(len(params_df)), 
    params_df['value'],
    yerr=errors,
    fmt='o',
    markersize=12,
    capsize=10,
    capthick=2.5,
    linewidth=2.5,
    ecolor='gray',
    markerfacecolor=colors[0],
    markeredgecolor='darkblue',
    markeredgewidth=2
)

ax.set_xticks(range(len(params_df)))
ax.set_xticklabels(params_df['name'], rotation=15, ha='right')
ax.set_xlabel('Parameter', fontsize=13, fontweight='bold')
ax.set_ylabel('Value', fontsize=13, fontweight='bold')
ax.set_title('Parameter Estimates with 95% Confidence Intervals', fontsize=15, fontweight='bold', pad=20)
ax.grid(True, alpha=0.3, linestyle='--')

plt.tight_layout()
plt.show()
```

---

## Goodness of Fit

```{python}
#| label: goodness-of-fit
#| fig-cap: "Model Predictions vs. True Values and Residual Analysis"

x, y_true, y_pred = generate_fit_data()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Scatter plot with regression line
ax1.scatter(y_true, y_pred, alpha=0.6, s=60, color='steelblue', edgecolor='navy', linewidth=0.5)
min_val, max_val = y_true.min(), y_true.max()
ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2.5, label='Perfect fit', alpha=0.8)
ax1.set_xlabel('True Values', fontsize=12, fontweight='bold')
ax1.set_ylabel('Predicted Values', fontsize=12, fontweight='bold')
ax1.set_title('Predicted vs. True Values', fontsize=14, fontweight='bold')
ax1.legend(fontsize=11, frameon=True, shadow=True)
ax1.grid(True, alpha=0.3, linestyle='--')

# Add R² annotation
r2_text = f"R² = {data['optimization_results']['r_squared']:.3f}"
ax1.text(0.05, 0.95, r2_text, transform=ax1.transAxes, 
         fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# Residual plot
residuals = y_pred - y_true
ax2.scatter(y_true, residuals, alpha=0.6, s=60, color='coral', edgecolor='darkred', linewidth=0.5)
ax2.axhline(y=0, color='r', linestyle='--', lw=2.5, alpha=0.8)
ax2.set_xlabel('True Values', fontsize=12, fontweight='bold')
ax2.set_ylabel('Residuals', fontsize=12, fontweight='bold')
ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, linestyle='--')

# Add zero-mean line indication
mean_resid = np.mean(residuals)
std_resid = np.std(residuals)
ax2.axhline(y=mean_resid, color='green', linestyle=':', lw=1.5, alpha=0.7, label=f'Mean: {mean_resid:.4f}')
ax2.axhline(y=mean_resid + 2*std_resid, color='orange', linestyle=':', lw=1.5, alpha=0.5)
ax2.axhline(y=mean_resid - 2*std_resid, color='orange', linestyle=':', lw=1.5, alpha=0.5)
ax2.legend(fontsize=10, frameon=True, shadow=True)

plt.tight_layout()
plt.show()
```

---

## Model Performance

### Training vs. Validation

```{python}
#| label: performance-comparison
#| fig-cap: "Training and Validation Performance Comparison"

fig, ax = plt.subplots(figsize=(8, 6))

categories = ['Training', 'Validation']
rmse_values = [
    data['optimization_results']['training_rmse'],
    data['optimization_results']['validation_rmse']
]

colors_perf = ['#2E86AB', '#A23B72']
bars = ax.bar(categories, rmse_values, color=colors_perf, width=0.5, edgecolor='black', linewidth=1.5)

ax.set_ylabel('RMSE', fontsize=13, fontweight='bold')
ax.set_title('Model Performance: Training vs. Validation', fontsize=15, fontweight='bold', pad=20)
ax.grid(True, axis='y', alpha=0.3, linestyle='--')

# Add value labels on bars
for bar, value in zip(bars, rmse_values):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{value:.4f}',
            ha='center', va='bottom', fontsize=12, fontweight='bold')

# Add reference line for acceptable error
ax.axhline(y=0.015, color='green', linestyle='--', lw=2, alpha=0.6, label='Target threshold')
ax.legend(fontsize=11, frameon=True, shadow=True)

plt.tight_layout()
plt.show()
```

### Statistical Quality Metrics

```{python}
#| label: quality-metrics
#| fig-cap: "Distribution of Quality Metrics"

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# AIC
axes[0].bar(['AIC'], [data['optimization_results']['aic']], color='#6A4C93', edgecolor='black', linewidth=1.5)
axes[0].set_ylabel('Value', fontsize=11, fontweight='bold')
axes[0].set_title('Akaike Information Criterion', fontsize=12, fontweight='bold')
axes[0].grid(True, axis='y', alpha=0.3)
axes[0].text(0, data['optimization_results']['aic']/2, 
             f"{data['optimization_results']['aic']:.2f}", 
             ha='center', va='center', fontsize=14, fontweight='bold', color='white')

# R²
axes[1].bar(['R²'], [data['optimization_results']['r_squared']], color='#1982C4', edgecolor='black', linewidth=1.5)
axes[1].set_ylabel('Value', fontsize=11, fontweight='bold')
axes[1].set_title('Coefficient of Determination', fontsize=12, fontweight='bold')
axes[1].set_ylim([0, 1])
axes[1].grid(True, axis='y', alpha=0.3)
axes[1].axhline(y=0.9, color='green', linestyle='--', lw=1.5, alpha=0.6, label='Excellent fit')
axes[1].legend(fontsize=9)
axes[1].text(0, data['optimization_results']['r_squared']/2, 
             f"{data['optimization_results']['r_squared']:.3f}", 
             ha='center', va='center', fontsize=14, fontweight='bold', color='white')

# Chi-squared
axes[2].bar(['χ²'], [data['goodness_of_fit']['chi_squared']], color='#FF6B6B', edgecolor='black', linewidth=1.5)
axes[2].set_ylabel('Value', fontsize=11, fontweight='bold')
axes[2].set_title('Chi-Squared Statistic', fontsize=12, fontweight='bold')
axes[2].grid(True, axis='y', alpha=0.3)
axes[2].text(0, data['goodness_of_fit']['chi_squared']/2, 
             f"{data['goodness_of_fit']['chi_squared']:.2f}", 
             ha='center', va='center', fontsize=14, fontweight='bold', color='white')

plt.tight_layout()
plt.show()
```

---

## Methods & Settings

**Software:** `{python} data['metadata']['software']`  
**Optimization Method:** Levenberg-Marquardt  
**Objective Function:** Least Squares  
**Convergence Criterion:** Relative tolerance < `{python} data['optimization_results']['convergence_criterion']`  
**Iterations to Convergence:** `{python} data['optimization_results']['iterations']`

**Data:**

- Training samples: `{python} data['goodness_of_fit']['training_points']`
- Validation samples: `{python} data['goodness_of_fit']['validation_points']`
- Degrees of freedom: `{python} data['goodness_of_fit']['degrees_of_freedom']`

**Model:**  `{python} data['metadata']['model_id']`

---

## Reproducibility Information

This report was automatically generated using:

- **sbmlsim:** `{python} data['metadata']['software']`
- **Quarto:** v1.8+
- **Python:** 3.10+
- **Libraries:** pandas, numpy, matplotlib, seaborn

**Report generated:** `{python} data['metadata']['timestamp']`  
**Generated by:** `{python} data['metadata']['user']`

### To reproduce these results:

```bash
# Run parameter optimization
sbmlsim optimize model.xml --params config.json --output results/

# Generate report
sbmlsim report generate results/ --template parameter_optimization
```

### Integration with sbmlsim

This template demonstrates the proof-of-concept for Issue [#180](https://github.com/matthiaskoenig/sbmlsim/issues/180) and GSoC project [#281](https://github.com/nrnb/GoogleSummerOfCode/issues/281).

**Proposed API:**

```python
from sbmlsim.reporting import generate_quarto_report

# After optimization
results = optimizer.optimize(model, data)

# Generate interactive HTML report
generate_quarto_report(
    results, 
    template='parameter_optimization',
    output='report.html',
    format='html'
)

# Generate static PDF report (via Typst)
generate_quarto_report(
    results, 
    template='parameter_optimization',
    output='report.pdf',
    format='pdf'
)
```

---

## Next Steps

This prototype demonstrates the following capabilities for Issue #180:

✅ **Template-based reporting** - Reusable Quarto template  
✅ **Interactive visualizations** - Embedded matplotlib/seaborn plots  
✅ **Self-contained output** - Single HTML file with all assets  
✅ **Reproducibility documentation** - Complete methods and settings  
✅ **Structured data input** - JSON format ready for Pydantic models  
✅ **Publication-ready quality** - Professional formatting and layout

**Future enhancements:**

- [ ] Add profile likelihood plots for parameter uncertainty
- [ ] Support sensitivity analysis results (heatmaps, summary tables)
- [ ] Create Typst templates for static PDF generation
- [ ] Add interactive plotly visualizations for web reports
- [ ] CI/CD integration for automated report generation
- [ ] GitHub Pages deployment workflow
- [ ] Support for SED-ML and PEtab workflows
- [ ] Literature review templates (PRISMA diagrams)
